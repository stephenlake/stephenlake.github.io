# Scaling persistent connections is hard.

10 February 2020

The company I work for utilises Pusher for websocket communications. That's mostly notifications sent from the server to the client whenever the end user should be made aware of something important triggered by another use within their organisation. Pusher is pretty expensive, but perfectly on par with other realtime providers and I had to know why. It's just a bunch of TCP connections, why do we have to pay so much for so little in return? Let the coding begin.

I started working on a plan to build a scalable TCP messaging network based on pusher.com's protocol because its so widely supported and the service we're currently using, so it'd make sense for convenient switch over, change the host on the client-side and you're done. The purpose of this project is to run tens of thousands of connections at low cost as opposed to paying insane amounts of money for the hard limits pusher provides. Realisticlly, we'd only need about 2,000 concurrent connections at maximum (at the time of writing), but I don't want to have to worry about updating the entire platform later, it should be able scale itself so I don't have to revisit this project any time soon.

The protocol is fairly straight forward, but quite verbose. It expects JSON on request and spits out JSON on response with the [caveat that some JSON payloads contain a stringified JSON payload within in it](https://pusher.com/docs/channels/library_auth_reference/pusher-websockets-protocol#example-json), why? Who knows, but the rest of the protocol is pretty self explanatory, this should be a piece of cake to simulate. After some Googling around trying to figure out the best stack to utilise for my desires I stumble across something amazing...

Low and behold... my favourite language and framework have already done exactly what I'm attempting to do, only at smaller scale. Presenting the [Laravel Websockets](https://github.com/beyondcode/laravel-websockets) package. All of the boilerplate already exists right there, an exact replica of the Pusher protocol just patiently waiting for someone to snatch and create a new provider out of it, at least that's the first thing that came to my mind. Finally a project I can take ownership of and satisfy my craving to provide something useful to the public.

The protocol is done, the server works, and all existing pusher clients successfully talk to each other. Great, protocol side of things aren't a problem. Now, scaling... Replication doesn't exist here within the package, there's no horizontal scaling included, understandably, but this is the part that matters most. How am I going to relay messages between servers. We must scale! Let the adventure begin!

Having little exposure to realtime scaling techniques, I did some reading up on RFC's for replication protocols. I spent the following week wasting time on trying to implement a mesh relay protocol similar to the beehive RFC for redundant failover. The idea would be that even if one server goes down, DNS or a loadbalancer would redirect all dropped clients to another server on the network, the problem comes in where we need to identify the origin and destination of that message because in that setup each server would need to know where each recipient is, it's not a difficult task, but a resource and time intensive one which in a world of *realtime* communication is critically important, especially where you're processing messages on a single threaded environment. I eventually managed to get this right, and it worked OKAY - the realization was that because it was a mesh with no single point of mapping, every single server would always need to be aware of every channel and user on the entire network, a memory consumption nightmare. Not feasiable, bad idea, facepalm, back to the drawing board.

Enter redis pub/sub replication. Publish and subscribe is something I've always been aware of, but never worked a project where it's been applicable. The basic concept is redis sits somewhere on your network waiting for servers to subscribe and/or publish events, rather than servers sending messages to each other, redis acts as a message stack where each server picks up a message as soon as it's ready and if relay is needed, publishes messages to that stack where others hear it - much like your traditional message queue except each server 'hears' new messages rather than polling for them. I swapped out all the mesh boilerplate code to instead publish messages to the redis instance and I was overjoyed with the results, it was a breath of fresh air, the simplicity was beautiful, I was in awe...

Still running all tests on my local machine using ports 80, 81, 82 for three websocket servers subscribed to a single redis replication host, things are looking up when suddenly... That log tail you have running in the background starts populating lines of exceptions followed by an abrupt halt. Sadness kicks in.

```
kernel: Out of memory: kill process 8791 (redis-server) score 49437 or a child
kernel: Killed process 8791 (redis-server)
```

Redis. Out of memory. FML.

To be continued.
